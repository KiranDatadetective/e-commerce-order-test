{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "356fcd10-4d20-43c1-b612-44ff26d9bb91",
   "metadata": {},
   "source": [
    "Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "782fac76-5b02-4bf8-b77b-7fba0cd6cb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Following packages are used for testing purposes\n",
    "import pytest\n",
    "import ipytest\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8ac572-84f3-4961-ad53-54d8501829ce",
   "metadata": {},
   "source": [
    "Call Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae43c525-303a-4102-9739-244b044d13cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.13.5\")\\\n",
    "        .config(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", \"D:/Programs/Real time project/real_time_streamer_pyspark/hive/warehouse\") \\\n",
    "        .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d96c5a-a952-4252-9873-55eb69b266ae",
   "metadata": {},
   "source": [
    "Read Data function using spark. raw data format, schema and dbfs path are parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90a63fac-0dc5-4495-9213-76c57c4f3203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(spark, format,schema, path):\n",
    "    if format == \"json\":\n",
    "        df = spark.read.format(format).schema(schema).option(\"multiline\", \"true\").load(path) #schema is specified for reading json data.\n",
    "    else:\n",
    "        df = spark.read.format(format).option(\"header\",True).option(\"inferSchema\",True).load(path) #Used default inferSchema option to read data in csv and xlsx format\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0b6480-7ec8-428e-a8c2-cf15a8b94a06",
   "metadata": {},
   "source": [
    "Spark Write Function on Hive table. Write mode, data format and table name are parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c51c3e5-ef92-4d2d-97e2-e91fe8e73356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_function(df, mode, format, table_name):\n",
    "    try:\n",
    "        df.write \\\n",
    "            .mode(mode) \\\n",
    "            .format(format) \\\n",
    "            .save(table_name)\n",
    "        print(f\"DataFrame is written to {table_name}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing the DataFrame: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e585c01a-0872-460f-82a0-403d21811124",
   "metadata": {},
   "source": [
    "Order enrichment function which is a joined dataset of Orders, Products and customers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0b3fa7b-d462-45b5-bb9b-7898105a587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profit column is rounded to 2 decimal points and Order Year column is created.\n",
    "def order_enrichment(df_orders, df_customers, df_products):\n",
    "    df_orders_sel = df_orders.withColumn(\"Order Year\", year(to_timestamp(col(\"Order Date\"), \"dd/mm/yyyy\")))\\\n",
    "                                                .withColumn(\"Profit\", round(col(\"Profit\"),2))\n",
    "    df_cust_sel = df_customers.select(col(\"Customer ID\"), col(\"Customer Name\"), col(\"Country\"))\n",
    "    df_prod_sel = df_products.select(col(\"Product ID\"), col(\"Category\"), col(\"Sub-Category\")).distinct()\n",
    "    \n",
    "    df_orders_enrich = df_orders_sel.join(df_cust_sel, [\"Customer ID\"], \"inner\").join(df_prod_sel, [\"Product ID\"], \"left\")\n",
    "    \n",
    "    return df_orders_enrich\n",
    "    \n",
    "# Bug with Product dataset\n",
    "# Duplicate Product IDs - Same Product ID have multiple records with different name and location\n",
    "# few missing Product ID's in product table but are in Orders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5155fa45-34c0-4c71-ba7b-5f2a746f8378",
   "metadata": {},
   "source": [
    "Function to aggregate the order_enriched table to calculate `Total Profit` based on different group by fields.\n",
    "Final result is ordered by `Total Profit` in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84085d31-905c-468f-abec-caf27c139525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profit(df_order_enriched, field):\n",
    "    df = df_order_enriched.groupBy(field).agg(sum(\"Profit\").alias(\"Total Profit\"))\n",
    "    df_round = df.withColumn(\"Total Profit\", round(col(\"Total Profit\"),2))\n",
    "    return df_round.orderBy(col(\"Total Profit\").desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80234ec7-b6fc-498a-802e-15c865c91924",
   "metadata": {},
   "source": [
    "I have used a TDD approach to develop the above functions based on the requirements provided. 2 Major functions are used and the same are tested using below functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94954665-ce81-4e70-aaaf-42d2c485e6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "# pytest is the package used for testing purpose and ipytest is the package used to enable testing features in the databricks notebook.\n",
    "\n",
    "\n",
    "# Testing function to test order_enrichment function\n",
    "\n",
    "def test_order_enrichment(spark=spark):\n",
    "    # dummy dataframes are created using spark which included necessary corner cases like multiple orders, customers, products, product table duplication, profit with long decimal point\n",
    "    df_orders_raw_list = [( \"1\",\"order_id1\",\"21/8/2016\",\"25/8/2016\",\"Standard Class\",\"cust_id1\",\"prod_id1\",\"7\",\"573.174\",\"0.3\",\"63.686\"),\n",
    "                          ( \"2\",\"order_id2\",\"21/8/2017\",\"25/8/2017\",\"First Class\",\"cust_id2\",\"prod_id2\",\"3\",\"573.174\",\"0.3\",\"50.112\")]\n",
    "    \n",
    "    df_customers_raw_list = [(\"cust_id1\",\"cust_name1\",\"email1\",\"00000000\",\"address1\",\"Consumer\",\"country1\",\"city1\",\"state1\",\"80027.0\",\"West\"),\n",
    "                           (\"cust_id2\",\"cust_name2\",\"email1\",\"00000000\",\"address2\",\"Consumer\",\"country2\",\"city2\",\"state2\",\"80027.0\",\"West\")]\n",
    "    \n",
    "    df_products_raw_list = [(\"prod_id1\",\"Furniture\",\"Chairs\",\"prod_name1\",\"New York\",\"81.882\"),\n",
    "                            (\"prod_id2\",\"Technology\",\"Accessories\",\"prod_name2\",\"New York\",\"81.882\"),\n",
    "                           (\"prod_id3\",\"Technology\",\"Accessories\",\"prod_name3\",\"New York\",\"81.882\")]\n",
    "    \n",
    "    df_orders_raw = spark.createDataFrame(df_orders_raw_list, [\"Row ID\",\"Order ID\",\"Order Date\",\"Ship Date\",\"Ship Mode\",\"Customer ID\",\"Product ID\",\"Quantity\",\"Price\",\"Discount\",\"Profit\"])\n",
    "    df_customers_raw = spark.createDataFrame(df_customers_raw_list, [\"Customer ID\",\"Customer Name\",\"email\",\"phone\",\"address\",\"Segment\",\"Country\",\"City\",\"State\",\"Postal Code\",\"Region\"])\n",
    "    df_products_raw = spark.createDataFrame(df_products_raw_list, [\"Product ID\",\"Category\",\"Sub-Category\",\"Product Name\",\"State\",\"Price per product\"])\n",
    "\n",
    "    \n",
    "    # dummy dataframe are created with expected results based on the input data.\n",
    "    df_enrichment_final_raw_list = [(\"prod_id1\",\"cust_id1\",\"1\",\"order_id1\",\"21/8/2016\",\"25/8/2016\",\"Standard Class\",\"7\",\"573.174\",\"0.3\",\"63.69\",\"2016\",\"cust_name1\",\"country1\",\"Furniture\",\"Chairs\"),\n",
    "                                   (\"prod_id2\",\"cust_id2\",\"2\",\"order_id2\",\"21/8/2017\",\"25/8/2017\",\"First Class\",\"3\",\"573.174\",\"0.3\",\"50.11\",\"2017\",\"cust_name2\",\"country2\",\"Technology\",\"Accessories\")]\n",
    "    \n",
    "    df_enrichment_final_expected = spark.createDataFrame(df_enrichment_final_raw_list,[\"Product ID\",\"Customer ID\",\"Row ID\",\"Order ID\",\"Order Date\",\"Ship Date\",\"Ship Mode\",\"Quantity\",\"Price\",\"Discount\",\"Profit\",\"Order Year\",\"Customer Name\",\"Country\",\"Category\",\"Sub-Category\"])\n",
    "\n",
    "    # Calling the order_enrichment function and execute based on our dummy dataframe and fetch the result\n",
    "    df_enrichment_final_result = order_enrichment(df_orders_raw, df_customers_raw, df_products_raw)\n",
    "\n",
    "    # Checking if result table count is equal to expected table count\n",
    "    assert df_orders_raw.count() == df_enrichment_final_result.count()\n",
    "\n",
    "    # Checking if result table have customer name based on the ID and matched with raw data. \n",
    "    # Also checked if the Profit is rounded to 2 decimal points\n",
    "    assert df_enrichment_final_result.select(col(\"Customer Name\"),col(\"Profit\")).where(col(\"Customer Name\")==\"cust_name1\").collect()[0][1] == \\\n",
    "           df_orders_raw.select(col(\"Customer ID\"),round(col(\"Profit\"),2)).where(col(\"Customer ID\")==\"cust_id1\").collect()[0][1]\n",
    "\n",
    "    # Checking if the result table is entirely equal to the dummy expected table\n",
    "    assert df_enrichment_final_result.orderBy(col(\"Row ID\").asc()).exceptAll(df_enrichment_final_expected.orderBy(col(\"Row ID\").asc())).count() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb563bf-3fff-4923-a03f-25c2352533cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest\n",
    "\n",
    "def test_profit(spark=spark):\n",
    "    # dummy data for order enriched table is created\n",
    "    df_order_enriched_raw_list = [(\"prod_id1\",\"cust_id1\",\"1\",\"order_id1\",\"21/8/2016\",\"25/8/2016\",\"Standard Class\",\"7\",\"573.174\",\"0.3\",\"63.69\",\"2016\",\"cust_name1\",\"country1\",\"Furniture\",\"Chairs\"),\n",
    "                                   (\"prod_id2\",\"cust_id2\",\"2\",\"order_id2\",\"21/8/2017\",\"25/8/2017\",\"First Class\",\"3\",\"573.174\",\"0.3\",\"50.11\",\"2017\",\"cust_name2\",\"country2\",\"Technology\",\"Accessories\"),\n",
    "                                   (\"prod_id3\",\"cust_id2\",\"3\",\"order_id3\",\"21/8/2017\",\"25/8/2017\",\"First Class\",\"3\",\"573.174\",\"0.3\",\"50.11\",\"2017\",\"cust_name2\",\"country2\",\"Technology\",\"Accessories\")]\n",
    "    \n",
    "    df_order_enriched_raw = spark.createDataFrame(df_order_enriched_raw_list,[\"Product ID\",\"Customer ID\",\"Row ID\",\"Order ID\",\"Order Date\",\"Ship Date\",\"Ship Mode\",\"Quantity\",\"Price\",\"Discount\",\"Profit\",\"Order Year\",\"Customer Name\",\"Country\",\"Category\",\"Sub-Category\"])\n",
    "    \n",
    "    # dummy data was created with expected results based on the input dummy data. \n",
    "    df_profit_per_year_final_expected = spark.createDataFrame([(\"2016\", \"63.69\"),(\"2017\", \"100.22\")], [\"Order Year\",\"Total Profit\"])\n",
    "    df_profit_per_category_final_expected = spark.createDataFrame([(\"Furniture\", \"63.69\"),(\"Technology\", \"100.22\")], [\"Category\",\"Total Profit\"])\n",
    "    df_profit_per_sub_category_final_expected = spark.createDataFrame([(\"Chairs\", \"63.69\"),(\"Accessories\", \"100.22\")], [\"Sub-Category\",\"Total Profit\"])\n",
    "    df_profit_per_customer_final_expected = spark.createDataFrame([(\"cust_name1\", \"63.69\"),(\"cust_name2\", \"100.22\")], [\"Customer Name\",\"Total Profit\"])\n",
    "\n",
    "    # Calling the profit function and execute based on our dummy raw  order enriched and fetch the result\n",
    "    df_profit_per_year_final_result = profit(df_order_enriched=df_order_enriched_raw, field=\"Order Year\")\n",
    "    df_profit_per_category_final_result = profit(df_order_enriched=df_order_enriched_raw, field=\"Category\")\n",
    "    df_profit_per_sub_category_final_result = profit(df_order_enriched=df_order_enriched_raw, field=\"Sub-Category\")\n",
    "    df_profit_per_customer_final_result = profit(df_order_enriched=df_order_enriched_raw, field=\"Customer Name\")\n",
    "\n",
    "    # Checking if the Total Profit in the aggregated function is equal to total profit in raw order data\n",
    "    assert df_profit_per_year_final_result.select(sum(\"Total Profit\")).collect()[0][0] == df_order_enriched_raw.select(sum(\"Profit\")).collect()[0][0]\n",
    "\n",
    "    # Checking if Result table is equal to Expected table. This is done for all 4 aggregations.\n",
    "    assert df_profit_per_year_final_result.exceptAll(df_profit_per_year_final_expected.orderBy(col(\"Total Profit\").desc())).count() == 0\n",
    "    assert df_profit_per_category_final_result.exceptAll(df_profit_per_category_final_expected.orderBy(col(\"Total Profit\").desc())).count() == 0\n",
    "    assert df_profit_per_sub_category_final_result.exceptAll(df_profit_per_sub_category_final_expected.orderBy(col(\"Total Profit\").desc())).count() == 0\n",
    "    assert df_profit_per_customer_final_result.exceptAll(df_profit_per_customer_final_expected.orderBy(col(\"Total Profit\").desc())).count() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a1b309-9864-4243-9547-585135b7f0de",
   "metadata": {},
   "source": [
    "Schema to read json data. I have used default inderschema option in spark to read data from other data formats like xlsx and csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e8e6611-1ff8-4f0f-b2ec-519b260dd83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"Row ID\", IntegerType(), True),\n",
    "                      StructField(\"Order ID\", StringType(), True),\n",
    "                      StructField(\"Order Date\", StringType(), True),\n",
    "                      StructField(\"Ship Date\", StringType(), True),\n",
    "                      StructField(\"Ship Mode\", StringType(), True),\n",
    "                      StructField(\"Customer ID\", StringType(), True),\n",
    "                      StructField(\"Product ID\", StringType(), True),\n",
    "                      StructField(\"Quantity\", IntegerType(), True),\n",
    "                      StructField(\"Price\", FloatType(), True),\n",
    "                      StructField(\"Discount\", FloatType(), True),\n",
    "                      StructField(\"Profit\", FloatType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5538c7c-ad12-437d-ab0b-ac207f6009f9",
   "metadata": {},
   "source": [
    "Read raw data using read_data function. Spark Session, data format, schema, path to file are parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b05d81d1-0320-43e4-8900-8056b3ec9b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers = read_data(spark, format=\"com.crealytics.spark.excel\", schema=None, path=\"D:/Programs/Real time project/real_time_streamer_pyspark/order_data/Customer.xlsx\")\n",
    "df_products = read_data(spark, format=\"csv\", schema=None, path=\"D:/Programs/Real time project/real_time_streamer_pyspark/order_data/Product.csv\")\n",
    "df_orders = read_data(spark, format=\"json\",schema=schema,path=\"D:/Programs/Real time project/real_time_streamer_pyspark/order_data/Order.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375add30-a8bc-49ef-83b6-42e07de0d47a",
   "metadata": {},
   "source": [
    "Data transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b5fa2ab-5943-4160-b023-038258239df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function `order_enrichment`. raw dataframes of orders, customers, products are specified as parameters.\n",
    "df_order_enriched = order_enrichment(df_orders, df_customers, df_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "facce6bc-f7a3-4ff6-85de-74cd31b042d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function `profit` to calulate profit by specific fields and stored as different dataframes. \n",
    "# df_order_enriched and field column (Year, Category, Sub-category, Customer Name) for grouping is specified.\n",
    "df_profit_by_year = profit(df_order_enriched=df_order_enriched, field=\"Order Year\")\n",
    "df_profit_by_product_category =  profit(df_order_enriched=df_order_enriched, field=\"Category\")\n",
    "df_profit_by_product_sub_category = profit(df_order_enriched=df_order_enriched, field=\"Sub-Category\")\n",
    "df_profit_by_customer = profit(df_order_enriched=df_order_enriched, field=\"Customer Name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2847f96-92fb-4d7d-a71f-8ebfcaabc7ce",
   "metadata": {},
   "source": [
    "Write data to tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9be05794-a7bd-49ec-986b-330bbe818ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame is written to Sink/Orders.\n",
      "DataFrame is written to Sink/Products.\n",
      "DataFrame is written to Sink/Customers.\n",
      "DataFrame is written to Sink/Orders_enriched.\n",
      "DataFrame is written to Sink/Profit_by_Year.\n",
      "DataFrame is written to Sink/Profit_by_Product_Category.\n",
      "DataFrame is written to Sink/Profit_by_Product_Sub_Category.\n",
      "DataFrame is written to Sink/Profit_by_Customer.\n"
     ]
    }
   ],
   "source": [
    "# Call function `write_function` to write the required dataframes to tables using Hive.\n",
    "# I have used parquet as dataformat to write on for better compressibility. \n",
    "# Respective Table name is also specified along with write mode as overwrite\n",
    "# Raw tables, order enriched table and profit by category tables are created.\n",
    "write_function(df=df_orders,  mode=\"overwrite\", format=\"parquet\", table_name=\"Sink/Orders\")\n",
    "write_function(df=df_products,  mode=\"overwrite\", format=\"parquet\", table_name=\"Sink/Products\")\n",
    "write_function(df=df_customers,  mode=\"overwrite\", format=\"parquet\", table_name=\"Sink/Customers\")\n",
    "\n",
    "write_function(df=df_order_enriched,  mode=\"overwrite\", format=\"parquet\", table_name=\"Sink/Orders_enriched\")\n",
    "\n",
    "write_function(df=df_profit_by_year,  mode=\"overwrite\", format=\"parquet\", table_name=\"Sink/Profit_by_Year\")\n",
    "write_function(df=df_profit_by_product_category,  mode=\"overwrite\", format=\"parquet\", table_name=\"Sink/Profit_by_Product_Category\")\n",
    "write_function(df=df_profit_by_product_sub_category,  mode=\"overwrite\", format=\"parquet\", table_name=\"Sink/Profit_by_Product_Sub_Category\")\n",
    "write_function(df=df_profit_by_customer,  mode=\"overwrite\", format=\"parquet\", table_name=\"Sink/Profit_by_Customer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3b3869-d3aa-4ba9-908c-05ee532d8163",
   "metadata": {},
   "source": [
    "SQL statement to fetch respective tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30373206-470e-4bbd-97b1-d2fff078a270",
   "metadata": {},
   "outputs": [],
   "source": [
    "Profit_by_Year_output = spark.read.format(\"parquet\").load(\"D:/Programs/Real time project/real_time_streamer_pyspark/Sink/Profit_by_Year\")\n",
    "Profit_by_Customer_output = spark.read.format(\"parquet\").load(\"D:/Programs/Real time project/real_time_streamer_pyspark/Sink/Profit_by_Customer\")\n",
    "Orders_enriched_output = spark.read.format(\"parquet\").load(\"D:/Programs/Real time project/real_time_streamer_pyspark/Sink/Orders_enriched\")\n",
    "\n",
    "Profit_by_Year_output.createOrReplaceTempView(\"Profit_by_Year\")\n",
    "Profit_by_Customer_output.createOrReplaceTempView(\"Profit_by_Customer\")\n",
    "Orders_enriched_output.createOrReplaceTempView(\"Orders_enriched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4e94185-fae6-41e5-9971-f657498af1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|Order Year|Total Profit|\n",
      "+----------+------------+\n",
      "|      2017|   111084.87|\n",
      "|      2016|    65073.28|\n",
      "|      2015|    63073.09|\n",
      "|      2014|    39185.71|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I wrote query inside spark.sql here, We can also switch databricks cell to SQL mode to write query without using spark \n",
    "\n",
    "# Query to fetch Profit by Year\n",
    "spark.sql(\"\"\"select * from Profit_by_Year;\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7770f1b8-98cd-44b4-9194-ef2c0fbb3af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|       Customer Name|Total Profit|\n",
      "+--------------------+------------+\n",
      "|        Frank Hawley|    13400.31|\n",
      "|        Tamara Chand|     8981.32|\n",
      "|        Raymond Buch|     6976.36|\n",
      "|        Sanjit Chand|      5757.3|\n",
      "|        Hunter Lopez|     5622.43|\n",
      "|        Patrick Ryan|      5596.2|\n",
      "|       Adrian Barton|     5444.97|\n",
      "|        Tom Ashbrook|     4703.72|\n",
      "|Christopher Martinez|     3900.04|\n",
      "|     Penelope Sewall|     3183.77|\n",
      "|                NULL|      3126.8|\n",
      "|       Keith Dawkins|     3038.92|\n",
      "|         Andy Reiter|     2884.61|\n",
      "|       Daniel Raglin|     2869.08|\n",
      "|    Tom Boeckenhauer|     2798.06|\n",
      "|        Nathan Mautz|      2751.7|\n",
      "|        Sanjit Engle|     2650.68|\n",
      "|    Bi 8761l Shonely|     2616.41|\n",
      "|         Harry Marie|     2438.07|\n",
      "|        Todd Sumrall|     2371.72|\n",
      "+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query to fetch Profit by Customer\n",
    "spark.sql(\"\"\"select * from Profit_by_Customer order by `Total Profit` desc;\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "85453f75-8662-4b51-9e12-abfcb8422f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+-------------------+\n",
      "|Order Year|       Category|        sum(Profit)|\n",
      "+----------+---------------+-------------------+\n",
      "|      2017|Office Supplies|  44273.04976409674|\n",
      "|      2017|     Technology|  63281.79999738932|\n",
      "|      2017|      Furniture| 3041.5200760364532|\n",
      "|      2017|           NULL|  488.5000163912773|\n",
      "|      2016|     Technology| 23223.780251443386|\n",
      "|      2016|           NULL|  404.4500068426132|\n",
      "|      2016|      Furniture|  6889.499873638153|\n",
      "|      2016|Office Supplies|  34555.55016118288|\n",
      "|      2015|           NULL|  583.1600017547607|\n",
      "|      2015|Office Supplies|  24519.38986013457|\n",
      "|      2015|      Furniture|  3027.169886946678|\n",
      "|      2015|     Technology|  34943.36974078417|\n",
      "|      2014|     Technology|  21493.33014243841|\n",
      "|      2014|           NULL|  523.1099948883057|\n",
      "|      2014|      Furniture|-5331.0600063204765|\n",
      "|      2014|Office Supplies| 22500.330029863864|\n",
      "+----------+---------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query to fetch Profit by Year and Category\n",
    "spark.sql(\"\"\"select `Order Year`, `Category`, sum(Profit) \n",
    "            from Orders_enriched \n",
    "            group by `Order Year`,`Category`\n",
    "            order by `Order Year` desc;\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7481f77d-0aab-4f66-b0d2-d06598854af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-------------------+\n",
      "|      Customer Name|Order Year|        sum(Profit)|\n",
      "+-------------------+----------+-------------------+\n",
      "|    Laura Armstrong|      2017| 16.070000380277634|\n",
      "|      Filia McAdams|      2017|-24.750000476837158|\n",
      "| Zuschuss Donatelli|      2017|  16.59000015258789|\n",
      "|       Jeremy Farry|      2017| 36.979999363422394|\n",
      "|       Greg Guthrie|      2017|-10.070001244544983|\n",
      "|          Janet Lee|      2017|  88.18999862670898|\n",
      "|           Amy Hunt|      2017| 147.86999988555908|\n",
      "|    Neola Schneider|      2017| -31.86000108718872|\n",
      "|       Sonia Cooley|      2017|  70.27000188827515|\n",
      "|    Erica Hernandez|      2017|  144.2900037765503|\n",
      "|     Lena Hernandez|      2017|  65.21999931335449|\n",
      "|       Lena Radford|      2017| -272.5799865722656|\n",
      "|     Pamela Coakley|      2017|-0.9900000095367432|\n",
      "|     Vicky Freymann|      2017|  62.47999882698059|\n",
      "|     Parhena Norris|      2017| 48.350001096725464|\n",
      "|Rick Wi 4567@#$lson|      2017| 1660.8500208854675|\n",
      "|      Edward Nazzal|      2017|  6.550000190734863|\n",
      "|    Candace McMahon|      2017| 185.92999696731567|\n",
      "|    Darrin Van Huff|      2017|-445.10999977588654|\n",
      "|  Steven Cartwright|      2017|  483.7700026035309|\n",
      "+-------------------+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query to fetch Profit by Customer and Year\n",
    "spark.sql(\"\"\"select `Customer Name`, `Order Year`, sum(Profit) \n",
    "             from Orders_enriched \n",
    "             group by `Customer Name`, `Order Year`\n",
    "             order by `Order Year` desc;\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee543a9-2d0b-4257-b636-d1b5a73a27b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
