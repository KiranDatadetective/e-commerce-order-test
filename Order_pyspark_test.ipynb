{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "356fcd10-4d20-43c1-b612-44ff26d9bb91",
   "metadata": {},
   "source": [
    "Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782fac76-5b02-4bf8-b77b-7fba0cd6cb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Following packages are used for testing purposes\n",
    "import pytest\n",
    "import ipytest\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8ac572-84f3-4961-ad53-54d8501829ce",
   "metadata": {},
   "source": [
    "Call Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae43c525-303a-4102-9739-244b044d13cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.13.5\")\\ # to read xlsx data\n",
    "        .config(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")\\ # to do operations on datetime\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d96c5a-a952-4252-9873-55eb69b266ae",
   "metadata": {},
   "source": [
    "Read Data function using spark. raw data format, schema and dbfs path are parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a63fac-0dc5-4495-9213-76c57c4f3203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(spark, format,schema, path):\n",
    "    if format == \"json\":\n",
    "        df = spark.read.format(format).schema(schema).option(\"multiline\", \"true\").load(path) #schema is specified for reading json data.\n",
    "    else:\n",
    "        df = spark.read.format(format).option(\"header\",True).option(\"inferSchema\",True).load(path) #Used default inferSchema option to read data in csv and xlsx format\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0b6480-7ec8-428e-a8c2-cf15a8b94a06",
   "metadata": {},
   "source": [
    "Spark Write Function on Hive table. Write mode, data format and table name are parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c51c3e5-ef92-4d2d-97e2-e91fe8e73356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_function(df, mode, format, table_name):\n",
    "    try:\n",
    "        df.write \\\n",
    "            .mode(mode) \\\n",
    "            .format(format) \\\n",
    "            .saveAsTable(table_name)\n",
    "        print(f\"DataFrame is written to {table_name}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing the DataFrame: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e585c01a-0872-460f-82a0-403d21811124",
   "metadata": {},
   "source": [
    "Order enrichment function which is a joined dataset of Orders, Products and customers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b3fa7b-d462-45b5-bb9b-7898105a587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profit column is rounded to 2 decimal points and Order Year column is created.\n",
    "def order_enrichment(df_orders, df_customers, df_products):\n",
    "    df_orders_sel = df_orders.withColumn(\"Order Year\", year(to_timestamp(col(\"Order Date\"), \"dd/mm/yyyy\")))\\\n",
    "                                                .withColumn(\"Profit\", round(col(\"Profit\"),2))\n",
    "    df_cust_sel = df_customers.select(col(\"Customer ID\"), col(\"Customer Name\"), col(\"Country\"))\n",
    "    df_prod_sel = df_products.select(col(\"Product ID\"), col(\"Category\"), col(\"Sub-Category\")).distinct()\n",
    "    \n",
    "    df_orders_enrich = df_orders_sel.join(df_cust_sel, [\"Customer ID\"], \"inner\").join(df_prod_sel, [\"Product ID\"], \"left\")\n",
    "    \n",
    "    return df_orders_enrich\n",
    "    \n",
    "# Bug with Product dataset\n",
    "# Duplicate Product IDs - Same Product ID have multiple records with different name and location\n",
    "# few missing Product ID's in product table but are in Orders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5155fa45-34c0-4c71-ba7b-5f2a746f8378",
   "metadata": {},
   "source": [
    "Function to aggregate the order_enriched table to calculate `Total Profit` based on different group by fields.\n",
    "Final result is ordered by `Total Profit` in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84085d31-905c-468f-abec-caf27c139525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profit(df_order_enriched, field):\n",
    "    df = df_order_enriched.groupBy(field).agg(sum(\"Profit\").alias(\"Total Profit\"))\n",
    "    df_round = df.withColumn(\"Total Profit\", round(col(\"Total Profit\"),2))\n",
    "    return df_round.orderBy(col(\"Total Profit\").desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a1b309-9864-4243-9547-585135b7f0de",
   "metadata": {},
   "source": [
    "Schema to read json data. I have used default inderschema option in spark to read data from other data formats like xlsx and csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8e6611-1ff8-4f0f-b2ec-519b260dd83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"Row ID\", IntegerType(), True),\n",
    "                      StructField(\"Order ID\", StringType(), True),\n",
    "                      StructField(\"Order Date\", StringType(), True),\n",
    "                      StructField(\"Ship Date\", StringType(), True),\n",
    "                      StructField(\"Ship Mode\", StringType(), True),\n",
    "                      StructField(\"Customer ID\", StringType(), True),\n",
    "                      StructField(\"Product ID\", StringType(), True),\n",
    "                      StructField(\"Quantity\", IntegerType(), True),\n",
    "                      StructField(\"Price\", FloatType(), True),\n",
    "                      StructField(\"Discount\", FloatType(), True),\n",
    "                      StructField(\"Profit\", FloatType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5538c7c-ad12-437d-ab0b-ac207f6009f9",
   "metadata": {},
   "source": [
    "Read raw data using read_data function. Spark Session, data format, schema, path to file are parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05d81d1-0320-43e4-8900-8056b3ec9b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers = read_data(spark, format=\"com.crealytics.spark.excel\", schema=None, path=\"dbfs:/FileStore/Customer.xlsx\")\n",
    "df_products = read_data(spark, format=\"csv\", schema=None, path=\"dbfs:/FileStore/Product.csv\")\n",
    "df_orders = read_data(spark, format=\"json\",schema=schema,path=\"dbfs:/FileStore/Order.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375add30-a8bc-49ef-83b6-42e07de0d47a",
   "metadata": {},
   "source": [
    "Data transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5fa2ab-5943-4160-b023-038258239df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function `order_enrichment`. raw dataframes of orders, customers, products are specified as parameters.\n",
    "df_order_enriched = order_enrichment(df_orders, df_customers, df_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facce6bc-f7a3-4ff6-85de-74cd31b042d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function `profit` to calulate profit by specific fields and stored as different dataframes. \n",
    "# df_order_enriched and field column (Year, Category, Sub-category, Customer Name) for grouping is specified.\n",
    "df_profit_by_year = profit(df_order_enriched=df_order_enriched, field=\"Order Year\")\n",
    "df_profit_by_product_category =  profit(df_order_enriched=df_order_enriched, field=\"Category\")\n",
    "df_profit_by_product_sub_category = profit(df_order_enriched=df_order_enriched, field=\"Sub-Category\")\n",
    "df_profit_by_customer = profit(df_order_enriched=df_order_enriched, field=\"Customer Name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2847f96-92fb-4d7d-a71f-8ebfcaabc7ce",
   "metadata": {},
   "source": [
    "Write data to tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be05794-a7bd-49ec-986b-330bbe818ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function `write_function` to write the required dataframes to tables using Hive.\n",
    "# I have used parquet as dataformat to write on for better compressibility. \n",
    "# Respective Table name is also specified along with write mode as overwrite\n",
    "# Raw tables, order enriched table and profit by category tables are created.\n",
    "write_function(df=df_orders,  mode=\"overwrite\", format=\"parquet\", table_name=\"Orders\")\n",
    "write_function(df=df_products,  mode=\"overwrite\", format=\"parquet\", table_name=\"Products\")\n",
    "write_function(df=df_customers,  mode=\"overwrite\", format=\"parquet\", table_name=\"Customers\")\n",
    "\n",
    "write_function(df=df_order_enriched,  mode=\"overwrite\", format=\"parquet\", table_name=\"Orders_enriched\")\n",
    "\n",
    "write_function(df=df_profit_by_year,  mode=\"overwrite\", format=\"parquet\", table_name=\"Profit_by_Year\")\n",
    "write_function(df=df_profit_by_product_category,  mode=\"overwrite\", format=\"parquet\", table_name=\"Profit_by_Product_Category\")\n",
    "write_function(df=df_profit_by_product_sub_category,  mode=\"overwrite\", format=\"parquet\", table_name=\"Profit_by_Product_Sub_Category\")\n",
    "write_function(df=df_profit_by_customer,  mode=\"overwrite\", format=\"parquet\", table_name=\"Profit_by_Customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f882710-bc15-41f6-8ea4-20207701fee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL statement to fetch respective tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e94185-fae6-41e5-9971-f657498af1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I wrote query inside spark.sql here, We can also switch databricks cell to SQL mode to write query without using spark \n",
    "\n",
    "# Query to fetch Profit by Year\n",
    "spark.sql(\"\"\"select * from default.Profit_by_Year;\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7770f1b8-98cd-44b4-9194-ef2c0fbb3af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to fetch Profit by Customer\n",
    "spark.sql(\"\"\"select * from default.Profit_by_Customer;\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85453f75-8662-4b51-9e12-abfcb8422f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to fetch Profit by Year and Category\n",
    "spark.sql(\"\"\"select sum(Profit) \n",
    "            from default.Orders_enriched \n",
    "            group by `Order Year`,`Category`\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7481f77d-0aab-4f66-b0d2-d06598854af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to fetch Profit by Customer and Year\n",
    "spark.sql(\"\"\"select sum(Profit) \n",
    "             from default.Orders_enriched \n",
    "             group by `Customer Name, `Order Year`;\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51adb925-1b67-4fe4-b840-5489b1631856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have used a TDD approach to develop the above functions based on the requirements provided. 2 Major functions are used and the same are tested using below functions.\n",
    "# pytest is the package used for testing purpose and ipytest is the package used to enable testing features in the databricks notebook.\n",
    "\n",
    "\n",
    "# Testing function to test order_enrichment function\n",
    "%%ipytest\n",
    "def test_order_enrichment(spark=spark):\n",
    "    # dummy dataframes are created using spark which included necessary corner cases like multiple orders, customers, products, product table duplication, profit with long decimal point\n",
    "    df_orders_raw_list = [( \"1\",\"order_id1\",\"21/8/2016\",\"25/8/2016\",\"Standard Class\",\"cust_id1\",\"prod_id1\",\"7\",\"573.174\",\"0.3\",\"63.686\"),\n",
    "                          ( \"2\",\"order_id2\",\"21/8/2017\",\"25/8/2017\",\"First Class\",\"cust_id2\",\"prod_id2\",\"3\",\"573.174\",\"0.3\",\"50.112\")]\n",
    "    \n",
    "    df_customers_raw_list = [(\"cust_id1\",\"cust_name1\",\"email1\",\"00000000\",\"address1\",\"Consumer\",\"country1\",\"city1\",\"state1\",\"80027.0\",\"West\"),\n",
    "                           (\"cust_id2\",\"cust_name2\",\"email1\",\"00000000\",\"address2\",\"Consumer\",\"country2\",\"city2\",\"state2\",\"80027.0\",\"West\")]\n",
    "    \n",
    "    df_products_raw_list = [(\"prod_id1\",\"Furniture\",\"Chairs\",\"prod_name1\",\"New York\",\"81.882\"),\n",
    "                            (\"prod_id2\",\"Technology\",\"Accessories\",\"prod_name2\",\"New York\",\"81.882\"),\n",
    "                           (\"prod_id3\",\"Technology\",\"Accessories\",\"prod_name3\",\"New York\",\"81.882\")]\n",
    "    \n",
    "    df_orders_raw = spark.createDataFrame(df_orders_raw_list, [\"Row ID\",\"Order ID\",\"Order Date\",\"Ship Date\",\"Ship Mode\",\"Customer ID\",\"Product ID\",\"Quantity\",\"Price\",\"Discount\",\"Profit\"])\n",
    "    df_customers_raw = spark.createDataFrame(df_customers_raw_list, [\"Customer ID\",\"Customer Name\",\"email\",\"phone\",\"address\",\"Segment\",\"Country\",\"City\",\"State\",\"Postal Code\",\"Region\"])\n",
    "    df_products_raw = spark.createDataFrame(df_products_raw_list, [\"Product ID\",\"Category\",\"Sub-Category\",\"Product Name\",\"State\",\"Price per product\"])\n",
    "\n",
    "    \n",
    "    # dummy dataframe are created with expected results based on the input data.\n",
    "    df_enrichment_final_raw_list = [(\"prod_id1\",\"cust_id1\",\"1\",\"order_id1\",\"21/8/2016\",\"25/8/2016\",\"Standard Class\",\"7\",\"573.174\",\"0.3\",\"63.69\",\"2016\",\"cust_name1\",\"country1\",\"Furniture\",\"Chairs\"),\n",
    "                                   (\"prod_id2\",\"cust_id2\",\"2\",\"order_id2\",\"21/8/2017\",\"25/8/2017\",\"First Class\",\"3\",\"573.174\",\"0.3\",\"50.11\",\"2017\",\"cust_name2\",\"country2\",\"Technology\",\"Accessories\")]\n",
    "    \n",
    "    df_enrichment_final_expected = spark.createDataFrame(df_enrichment_final_raw_list,[\"Product ID\",\"Customer ID\",\"Row ID\",\"Order ID\",\"Order Date\",\"Ship Date\",\"Ship Mode\",\"Quantity\",\"Price\",\"Discount\",\"Profit\",\"Order Year\",\"Customer Name\",\"Country\",\"Category\",\"Sub-Category\"])\n",
    "\n",
    "    # Calling the order_enrichment function and execute based on our dummy dataframe and fetch the result\n",
    "    df_enrichment_final_result = order_enrichment(df_orders_raw, df_customers_raw, df_products_raw)\n",
    "\n",
    "    # Checking if result table count is equal to expected table count\n",
    "    assert df_orders_raw.count() == df_enrichment_final_result.count()\n",
    "\n",
    "    # Checking if result table have customer name based on the ID and matched with raw data. \n",
    "    # Also checked if the Profit is rounded to 2 decimal points\n",
    "    assert df_enrichment_final_result.select(col(\"Customer Name\"),col(\"Profit\")).where(col(\"Customer Name\")==\"cust_name1\").collect()[0][1] == \\\n",
    "           df_orders_raw.select(col(\"Customer ID\"),round(col(\"Profit\"),2)).where(col(\"Customer ID\")==\"cust_id1\").collect()[0][1]\n",
    "\n",
    "    # Checking if the result table is entirely equal to the dummy expected table\n",
    "    assert df_enrichment_final_result.orderBy(col(\"Row ID\").asc()).exceptAll(df_enrichment_final_expected.orderBy(col(\"Row ID\").asc())).count() == 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9f8ec0-9c77-4988-86a6-5bbd56d79fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing function to test Profit function\n",
    "%%ipytest\n",
    "\n",
    "def test_profit(spark=spark):\n",
    "    # dummy data for order enriched table is created\n",
    "    df_order_enriched_raw_list = [(\"prod_id1\",\"cust_id1\",\"1\",\"order_id1\",\"21/8/2016\",\"25/8/2016\",\"Standard Class\",\"7\",\"573.174\",\"0.3\",\"63.69\",\"2016\",\"cust_name1\",\"country1\",\"Furniture\",\"Chairs\"),\n",
    "                                   (\"prod_id2\",\"cust_id2\",\"2\",\"order_id2\",\"21/8/2017\",\"25/8/2017\",\"First Class\",\"3\",\"573.174\",\"0.3\",\"50.11\",\"2017\",\"cust_name2\",\"country2\",\"Technology\",\"Accessories\"),\n",
    "                                   (\"prod_id3\",\"cust_id2\",\"3\",\"order_id3\",\"21/8/2017\",\"25/8/2017\",\"First Class\",\"3\",\"573.174\",\"0.3\",\"50.11\",\"2017\",\"cust_name2\",\"country2\",\"Technology\",\"Accessories\")]\n",
    "    \n",
    "    df_order_enriched_raw = spark.createDataFrame(df_order_enriched_raw_list,[\"Product ID\",\"Customer ID\",\"Row ID\",\"Order ID\",\"Order Date\",\"Ship Date\",\"Ship Mode\",\"Quantity\",\"Price\",\"Discount\",\"Profit\",\"Order Year\",\"Customer Name\",\"Country\",\"Category\",\"Sub-Category\"])\n",
    "    \n",
    "    # dummy data was created with expected results based on the input dummy data. \n",
    "    df_profit_per_year_final_expected = spark.createDataFrame([(\"2016\", \"63.69\"),(\"2017\", \"100.22\")], [\"Order Year\",\"Total Profit\"])\n",
    "    df_profit_per_category_final_expected = spark.createDataFrame([(\"Furniture\", \"63.69\"),(\"Technology\", \"100.22\")], [\"Category\",\"Total Profit\"])\n",
    "    df_profit_per_sub_category_final_expected = spark.createDataFrame([(\"Chairs\", \"63.69\"),(\"Accessories\", \"100.22\")], [\"Sub-Category\",\"Total Profit\"])\n",
    "    df_profit_per_customer_final_expected = spark.createDataFrame([(\"cust_name1\", \"63.69\"),(\"cust_name2\", \"100.22\")], [\"Customer Name\",\"Total Profit\"])\n",
    "\n",
    "    # Calling the profit function and execute based on our dummy raw  order enriched and fetch the result\n",
    "    df_profit_per_year_final_result = profit(df_order_enriched=df_order_enriched_raw, field=\"Order Year\")\n",
    "    df_profit_per_category_final_result = profit(df_order_enriched=df_order_enriched_raw, field=\"Category\")\n",
    "    df_profit_per_sub_category_final_result = profit(df_order_enriched=df_order_enriched_raw, field=\"Sub-Category\")\n",
    "    df_profit_per_customer_final_result = profit(df_order_enriched=df_order_enriched_raw, field=\"Customer Name\")\n",
    "\n",
    "    # Checking if the Total Profit in the aggregated function is equal to total profit in raw order data\n",
    "    assert df_profit_per_year_final_result.select(sum(\"Total Profit\")).collect()[0][0] == df_order_enriched_raw.select(sum(\"Profit\")).collect()[0][0]\n",
    "\n",
    "    # Checking if Result table is equal to Expected table. This is done for all 4 aggregations.\n",
    "    assert df_profit_per_year_final_result.exceptAll(df_profit_per_year_final_expected.orderBy(col(\"Total Profit\").desc())).count() == 0\n",
    "    assert df_profit_per_category_final_result.exceptAll(df_profit_per_category_final_expected.orderBy(col(\"Total Profit\").desc())).count() == 0\n",
    "    assert df_profit_per_sub_category_final_result.exceptAll(df_profit_per_sub_category_final_expected.orderBy(col(\"Total Profit\").desc())).count() == 0\n",
    "    assert df_profit_per_customer_final_result.exceptAll(df_profit_per_customer_final_expected.orderBy(col(\"Total Profit\").desc())).count() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300f3034-21a2-49e0-b1a8-0b4b535681c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
